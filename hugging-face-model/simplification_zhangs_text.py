# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lbc_a8uxzvNO97N3cFqqhyQgSYqvShqt
"""

# Mount Google Drive and setup environment
from google.colab import drive
drive.mount('/content/drive')


import tensorflow as tf
import numpy as np
import os
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import deque
import random
import pickle
import matplotlib.pyplot as plt

#working directory
work_dir = '/content/drive/MyDrive/text_simplification'
if not os.path.exists(work_dir):
    os.makedirs(work_dir)
os.chdir(work_dir)

#directories for model saving
model_dir = os.path.join(work_dir, 'saved_models')
checkpoint_dir = os.path.join(work_dir, 'checkpoints')
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Check GPU availability
print("GPU Available: ", tf.test.is_gpu_available())
print("GPU Device Name: ", tf.test.gpu_device_name())

#Configure GPU memory growth
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("Memory growth enabled")
    except RuntimeError as e:
        print(f"Memory growth setting failed: {e}")

class Config:
    def __init__(self):
        # Model Parameters
        self.embedding_dim = 256
        self.hidden_units = 512
        self.vocab_size = None
        self.max_length = 50

        # Training Parameters
        self.batch_size = 32
        self.epochs = 50
        self.learning_rate = 0.001
        self.patience = 5

        # Paths
        self.data_dir = '/content/'
        self.model_dir = model_dir
        self.checkpoint_dir = checkpoint_dir

config = Config()


# Model body

class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, hidden_units):
        super(Encoder, self).__init__()
        self.hidden_units = hidden_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.lstm = tf.keras.layers.LSTM(hidden_units,
                                       return_sequences=True,
                                       return_state=True)

    def call(self, x, hidden=None):
        x = self.embedding(x)
        output, state_h, state_c = self.lstm(x, initial_state=hidden)
        return output, state_h, state_c

class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units)
        self.W2 = tf.keras.layers.Dense(units)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        query_with_time_axis = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(
            self.W1(query_with_time_axis) + self.W2(values)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, hidden_units):
        super(Decoder, self).__init__()
        self.hidden_units = hidden_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.lstm = tf.keras.layers.LSTM(hidden_units,
                                       return_sequences=True,
                                       return_state=True)
        self.attention = BahdanauAttention(hidden_units)
        self.fc = tf.keras.layers.Dense(vocab_size)

    def call(self, x, hidden, enc_output):
        context_vector, attention_weights = self.attention(hidden[0], enc_output)
        x = self.embedding(x)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
        output, state_h, state_c = self.lstm(x)
        output = tf.reshape(output, (-1, output.shape[2]))
        x = self.fc(output)
        return x, [state_h, state_c], attention_weights

from tensorflow.keras.saving import register_keras_serializable
@register_keras_serializable()
class TextSimplificationModel(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, hidden_units):
        super(TextSimplificationModel, self).__init__()
        self.encoder = Encoder(vocab_size, embedding_dim, hidden_units)
        self.decoder = Decoder(vocab_size, embedding_dim, hidden_units)

    def call(self, inputs, training=False):
        input_seq, target_seq = inputs
        enc_output, state_h, state_c = self.encoder(input_seq)
        decoder_state = [state_h, state_c]

        predictions = []
        for t in range(target_seq.shape[1]):
            decoder_input = tf.expand_dims(target_seq[:, t], 1)
            prediction, decoder_state, _ = self.decoder(
                decoder_input, decoder_state, enc_output)
            predictions.append(prediction)

        return tf.stack(predictions, axis=1)

#model saving utilities (checkpoints if exist)
def save_model_checkpoint(model, epoch, optimizer, loss, sari, prefix='checkpoint'):
    """Save model checkpoint with all training state"""
    checkpoint_path = os.path.join(config.checkpoint_dir, f'{prefix}_epoch_{epoch}')

    # Save model weights
    model.save_weights(checkpoint_path + '_weights.h5')

    # Save training state
    training_state = {
        'epoch': epoch,
        'optimizer_weights': optimizer.get_weights(),
        'loss': loss,
        'sari': sari
    }
    np.save(checkpoint_path + '_state.npy', training_state)
    print(f"Checkpoint saved: {checkpoint_path}")

def load_model_checkpoint(model, optimizer, prefix='checkpoint', epoch=None):
    """Load model checkpoint and training state"""
    if epoch is None:
        #find the  last checkpoint
        checkpoints = [f for f in os.listdir(config.checkpoint_dir) if f.endswith('_weights.h5')]
        if not checkpoints:
            print("No checkpoints found.")
            return 0, 0, 0

        latest = max(checkpoints, key=lambda x: int(x.split('_epoch_')[1].split('_')[0]))
        epoch = int(latest.split('_epoch_')[1].split('_')[0])

    checkpoint_path = os.path.join(config.checkpoint_dir, f'{prefix}_epoch_{epoch}')

    # model weights
    model.load_weights(checkpoint_path + '_weights.h5')

    #load training state
    training_state = np.load(checkpoint_path + '_state.npy', allow_pickle=True).item()
    optimizer.set_weights(training_state['optimizer_weights'])

    print(f"Loaded checkpoint from epoch {epoch}")
    return epoch, training_state['loss'], training_state['sari']


#data loading & text preprocessing 

def load_and_preprocess_data():
    """Load and preprocess the WikiLarge dataset"""
    def load_file(filename):
        with open(os.path.join(config.data_dir, filename), 'r', encoding='utf-8') as f:
            return [line.strip() for line in f.readlines()]

    print("Loading data files...")
    train_src = load_file('wiki.full.aner.train.src')
    train_dst = load_file('wiki.full.aner.train.dst')
    valid_src = load_file('wiki.full.aner.valid.src')
    valid_dst = load_file('wiki.full.aner.valid.dst')
    test_src = load_file('wiki.full.aner.test.src')
    test_dst = load_file('wiki.full.aner.test.dst')

    print(f"Loaded {len(train_src)} training pairs")
    print(f"Loaded {len(valid_src)} validation pairs")
    print(f"Loaded {len(test_src)} test pairs")

    #tokenizer 
    tokenizer = Tokenizer(oov_token='<UNK>')
    tokenizer.fit_on_texts(train_src + train_dst + valid_src + valid_dst)
    config.vocab_size = len(tokenizer.word_index) + 1

    def prepare_data(src_texts, dst_texts):
        src_seqs = tokenizer.texts_to_sequences(src_texts)
        dst_seqs = tokenizer.texts_to_sequences(dst_texts)

        src_padded = pad_sequences(src_seqs, maxlen=config.max_length, padding='post')
        dst_padded = pad_sequences(dst_seqs, maxlen=config.max_length, padding='post') #post paddding for equal dimensions

        return src_padded, dst_padded

    #print("Preparing datasets...")
    train_data = prepare_data(train_src, train_dst)
    valid_data = prepare_data(valid_src, valid_dst)
    test_data = prepare_data(test_src, test_dst)

    return train_data, valid_data, test_data, tokenizer


# SARI metric for model evaluation and generalization

def calculate_sari(orig_sentences, pred_sentences, ref_sentences):
    def get_ngrams(sentence, n):
        words = sentence.lower().split()
        return set(' '.join(words[i:i+n]) for i in range(len(words)-n+1))

    def compute_precision_recall_f1(pred_ngrams, ref_ngrams):
        if not pred_ngrams or not ref_ngrams:
            return 0.0
        intersection = len(pred_ngrams & ref_ngrams)
        precision = intersection / len(pred_ngrams) if pred_ngrams else 0.0
        recall = intersection / len(ref_ngrams) if ref_ngrams else 0.0
        if precision + recall == 0:
            return 0.0
        return 2 * precision * recall / (precision + recall)

    scores = []
    
    for orig, pred, ref in zip(orig_sentences, pred_sentences, ref_sentences):
        score = 0
        for n in range(1, 5):  #1-4 grams
            orig_ngrams =get_ngrams(orig, n)
            pred_ngrams= get_ngrams(pred, n)
            ref_ngrams = get_ngrams(ref, n)
            add_score = compute_precision_recall_f1(
                pred_ngrams -orig_ngrams,
                ref_ngrams- orig_ngrams
            )
            keep_score = compute_precision_recall_f1(
                pred_ngrams&orig_ngrams,
                ref_ngrams &orig_ngrams
            )
            del_score =compute_precision_recall_f1(
                orig_ngrams -pred_ngrams,
                orig_ngrams - ref_ngrams
            )

            score += (add_score+keep_score +del_score) / 3

        scores.append(score / 4)  #average 

    return np.mean(scores) * 100

class RLAgent:
    def __init__(self, vocab_size, embedding_dim, hidden_units, max_length):
        self.vocab_size = vocab_size
        self.max_length = max_length
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  #discount rate
        self.epsilon = 1.0  #exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            # Return an index within the vocabulary size
            return np.random.randint(self.vocab_size)
        state_tensor = tf.convert_to_tensor([state])
        act_values = model((state_tensor, state_tensor))
        return tf.argmax(act_values[0]).numpy()

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return

        minibatch = random.sample(self.memory, batch_size)
        states = []
        targets = []

        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                state_tensor = tf.convert_to_tensor([next_state])
                next_pred = model((state_tensor, state_tensor))
                target = reward + self.gamma * tf.reduce_max(next_pred[0]).numpy()

            state_tensor = tf.convert_to_tensor([state])
            target_f = model((state_tensor, state_tensor))
            target_f = target_f.numpy()

            # Ensure action is within bounds before indexing
            action = np.clip(action, 0, target_f.shape[1] - 1)
            target_f[0][action] = target

            states.append(state)
            targets.append(target_f[0])

        states = np.array(states)
        targets = np.array(targets)

        with tf.GradientTape() as tape:
            predictions = model((states, states))
            loss = tf.reduce_mean(tf.keras.losses.MSE(targets, predictions))

        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


# LEXICAL-SEMANTIC LOSS

class LexicalSemanticLoss(tf.keras.layers.Layer):
    def __init__(self, vocab_size, embedding_dim):
        super(LexicalSemanticLoss, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.word_embeddings = None

    def build_embeddings(self, texts, batch_size=64):
        print("Building lexical-semantic embeddings...")
        try:
            sequences = tokenizer.texts_to_sequences(texts)
            sequences = pad_sequences(sequences, maxlen=config.max_length, padding='post')

            word_vectors = {}
            num_sequences = len(sequences)

            for i in range(0, num_sequences, batch_size):
                batch_end = min(i + batch_size, num_sequences)
                batch_sequences = sequences[i:batch_end]
                batch_tensor = tf.convert_to_tensor(batch_sequences, dtype=tf.int32)
                batch_embeddings = model.encoder.embedding(batch_tensor)

                for seq, embeds in zip(batch_sequences, batch_embeddings.numpy()):
                    for word_idx, embed in zip(seq, embeds):
                        if word_idx == 0:  # Skip padding
                            continue
                        if word_idx in word_vectors:
                            word_vectors[word_idx].append(embed)
                        else:
                            word_vectors[word_idx] = [embed]

                if (i // batch_size) % 100 == 0:
                    print(f"Processed {i}/{num_sequences} sequences...")

            print("Calculating final embeddings...")
            averaged_embeddings = np.zeros((self.vocab_size, self.embedding_dim))
            for word_idx, embeds in word_vectors.items():
                averaged_embeddings[word_idx] = np.mean(embeds, axis=0)

            self.word_embeddings = tf.convert_to_tensor(averaged_embeddings, dtype=tf.float32)
            print("Lexical-semantic embeddings built successfully!")

        except Exception as e:
            print(f"Error building embeddings: {str(e)}")
            raise

    def call(self, target, prediction):
        target_one_hot = tf.one_hot(target, depth=self.vocab_size)
        target_embed = tf.matmul(target_one_hot, self.word_embeddings)
        pred_probs = tf.nn.softmax(prediction, axis=-1)
        pred_embed = tf.matmul(pred_probs, self.word_embeddings)

        target_embed = tf.nn.l2_normalize(target_embed, axis=-1)
        pred_embed = tf.nn.l2_normalize(pred_embed, axis=-1)

        cosine_sim = tf.reduce_sum(target_embed * pred_embed, axis=-1)
        return 1.0 - tf.reduce_mean(cosine_sim)


# loss fns

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_mean(loss_)

def train_epoch(train_data, valid_data, rl_agent,lex_sem_loss):
    train_src, train_tgt = train_data
    train_loss= tf.keras.metrics.Mean()
    train_sari =tf.keras.metrics.Mean()

    dataset = tf.data.Dataset.from_tensor_slices((train_src, train_tgt))
    dataset = dataset.shuffle(1000).batch(config.batch_size)

    for batch, (src, tgt) in enumerate(dataset):
        
        loss = train_step(src, tgt)

        #generating simplifications for RL
        predictions = model((src, tgt[:, :-1]))
        simplified =tf.argmax(predictions, axis=-1)

        #calculate SARI rewards when model performs correctly or prdicts correctly
        pred_texts= tokenizer.sequences_to_texts(simplified.numpy())
        src_texts =tokenizer.sequences_to_texts(src.numpy())
        tgt_texts = tokenizer.sequences_to_texts(tgt.numpy())

        #calculating rewards for each sequence in the batch
        rewards = []
        for s, p, t in zip(src_texts, pred_texts, tgt_texts):
            reward =calculate_sari([s], [p], [t])
            rewards.append(reward)
        rewards = np.array(rewards)

        # store experiences in RL agent's memory
        for i in range(len(src)):
            rl_agent.remember(
                src[i].numpy(),
                simplified[i].numpy(),
                rewards[i],
                tgt[i].numpy(),
                True
            )

        #RL training step
        if batch % 10 == 0:
            rl_agent.replay(32)

        #lexical-semantic loss adddition
        lex_sem = lex_sem_loss(tgt[:, 1:], predictions)

        #cast rewards to float32 before subtraction floattype=32
        total_loss = loss + 0.1 * lex_sem - 0.01 * tf.reduce_mean(tf.cast(rewards, tf.float32))

        train_loss(total_loss)
        train_sari(np.mean(rewards))

        if batch % 100 == 0:
            print(f'Batch {batch}, Loss: {train_loss.result():.4f}, '
                  f'SARI: {train_sari.result():.2f}, '
                  f'Epsilon: {rl_agent.epsilon:.2f}')

    return train_loss.result(), train_sari.result()

def evaluate_model(model, data, batch_size=32):
    src_data, tgt_data = data
    total_loss=[]
    total_sari =[]

    for i in range(0, len(src_data), batch_size):
        src_batch =src_data[i:i +batch_size]
        tgt_batch = tgt_data[i:i+ batch_size]

        predictions = model((src_batch,tgt_batch[:, :-1]),training=False)
        loss = loss_function(tgt_batch[:, 1:], predictions)
        total_loss.append(loss)

        pred_texts = tokenizer.sequences_to_texts(
            tf.argmax(predictions, axis=-1).numpy())
        src_texts =tokenizer.sequences_to_texts(src_batch)
        tgt_texts= tokenizer.sequences_to_texts(tgt_batch)

        sari =calculate_sari(src_texts, pred_texts, tgt_texts)
        total_sari.append(sari)

    return np.mean(total_loss),np.mean(total_sari)

# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================
@tf.function
def train_step(input_seq, target_seq):
    """Single training step"""
    with tf.GradientTape() as tape:
        predictions = model((input_seq, target_seq[:, :-1]))
        loss = loss_function(target_seq[:, 1:], predictions)

    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

    return loss

def plot_training_history(history):
    """Plot training metrics"""
    plt.figure(figsize=(12, 4))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot SARI
    plt.subplot(1, 2, 2)
    plt.plot(history['train_sari'], label='Train SARI')
    plt.plot(history['val_sari'], label='Val SARI')
    plt.title('SARI Score')
    plt.xlabel('Epoch')
    plt.ylabel('SARI')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Save plot
    plt.savefig(os.path.join(config.model_dir, 'training_history.png'))

# ============================================================================
# LOAD DATA
# ============================================================================
print("Loading and preprocessing data...")
train_data, valid_data, test_data, tokenizer = load_and_preprocess_data()
print("Data loading completed!")

# ============================================================================
# MAIN EXECUTION
# ============================================================================
if __name__ == "__main__":
    try:
        # Initialize model and components
        print("Initializing model...")
        model = TextSimplificationModel(
            vocab_size=config.vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_units=config.hidden_units
        )

        optimizer = tf.keras.optimizers.Adam(config.learning_rate)

        print("Initializing RL agent and lexical-semantic loss...")
        rl_agent = RLAgent(config.vocab_size, config.embedding_dim,config.hidden_units, config.max_length)
        lex_sem_loss = LexicalSemanticLoss(config.vocab_size, config.embedding_dim)

        # Prepare embeddings
        print("Preparing texts for embedding building...")
        src_texts = tokenizer.sequences_to_texts(train_data[0])
        tgt_texts = tokenizer.sequences_to_texts(train_data[1])
        all_texts = src_texts + tgt_texts

        print(f"Building embeddings from {len(all_texts)} texts...")
        lex_sem_loss.build_embeddings(all_texts)

        # Initialize training history
        history = {
            'train_loss': [],
            'train_sari': [],
            'val_loss': [],
            'val_sari': []
        }

        # Load checkpoint if exists
        try:
            start_epoch, last_loss, last_sari = load_model_checkpoint(model, optimizer)
        except:
            start_epoch = 0
            last_loss = float('inf')
            last_sari = 0

        # Training loop
        print("Starting training...")
        best_sari = last_sari
        patience_counter = 0

        for epoch in range(start_epoch, config.epochs):
            print(f'\nEpoch {epoch+1}/{config.epochs}')

            # Training
            train_loss, train_sari = train_epoch(train_data, valid_data, rl_agent, lex_sem_loss)

            # Validation
            val_loss, val_sari = evaluate_model(model, valid_data)

            # Update history
            history['train_loss'].append(train_loss)
            history['train_sari'].append(train_sari)
            history['val_loss'].append(val_loss)
            history['val_sari'].append(val_sari)

            print(f'Epoch {epoch+1}:')
            print(f'Train Loss: {train_loss:.4f}, Train SARI: {train_sari:.2f}')
            print(f'Val Loss: {val_loss:.4f}, Val SARI: {val_sari:.2f}')

            # Save checkpoint
            if (epoch + 1) % 5 == 0:
                save_model_checkpoint(model, epoch + 1, optimizer, val_loss, val_sari)

            # Save best model
            if val_sari > best_sari:
                best_sari = val_sari
                save_model_checkpoint(model, epoch + 1, optimizer, val_loss, val_sari, prefix='best')
                print(f"New best SARI: {best_sari:.2f} - Model saved!")
                patience_counter = 0
            else:
                patience_counter += 1

            # Early stopping
            if patience_counter >= config.patience:
                print("Early stopping triggered!")
                break

        #final evaluation
        print("\nTraining completed! Evaluating on test set...")
        test_loss, test_sari = evaluate_model(model, test_data)
        print(f'Final Test Results:')
        print(f'Loss: {test_loss:.4f}')
        print(f'SARI: {test_sari:.2f}')

       
        plot_training_history(history)

        #saving model
        model.save(os.path.join(config.model_dir, 'final_model'))
        print("Final model saved successfully!")

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise


#BUILD AND SAVE EMBEDDINGS

if __name__ == "__main__":
    try:
        # Load data and initialize components
        print("Loading and preprocessing data...")
        train_data, valid_data, test_data, tokenizer = load_and_preprocess_data()
        print("Data loading completed!")

        print("Initializing model...")
        model = TextSimplificationModel(
            vocab_size=config.vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_units=config.hidden_units
        )

        # Prepare embeddings
        print("Preparing texts for embedding building...")
        src_texts = tokenizer.sequences_to_texts(train_data[0])
        tgt_texts = tokenizer.sequences_to_texts(train_data[1])
        all_texts = src_texts + tgt_texts
        print(f"Building embeddings from {len(all_texts)} texts...")

        lex_sem_loss = LexicalSemanticLoss(config.vocab_size, config.embedding_dim)
        lex_sem_loss.build_embeddings(all_texts)

        # Save the embeddings
        embedding_path = os.path.join(config.model_dir, 'embeddings.pkl')
        with open(embedding_path, 'wb') as f:
            pickle.dump(lex_sem_loss.word_embeddings, f)
        print("Embeddings built and saved successfully!")

    except Exception as e:
        print(f"An error occurred during embedding building: {str(e)}")
        raise


#Training Execution

if __name__ == "__main__":
    try:
        
        print("Loading and preprocessing data...")
        train_data, valid_data, test_data, tokenizer = load_and_preprocess_data()
        print("Data loading completed!")

        print("Initializing model...")
        model = TextSimplificationModel(
            vocab_size=config.vocab_size,
            embedding_dim=config.embedding_dim,
            hidden_units=config.hidden_units
        )

        optimizer = tf.keras.optimizers.Adam(config.learning_rate)

        print("Initializing RL agent and lexical-semantic loss...")
        rl_agent = RLAgent(config.vocab_size, config.embedding_dim, config.hidden_units, config.max_length)

        # Load embeddings
        embedding_path = os.path.join(config.model_dir, 'embeddings.pkl')
        with open(embedding_path, 'rb') as f:
            word_embeddings = pickle.load(f)

        lex_sem_loss = LexicalSemanticLoss(config.vocab_size, config.embedding_dim)
        lex_sem_loss.word_embeddings = word_embeddings  # Set the loaded embeddings
        print("Embeddings loaded successfully!")

        # Initialize training history
        history = {
            'train_loss': [],
            'train_sari': [],
            'val_loss': [],
            'val_sari': []
        }

        # Load checkpoint if exists
        try:
            start_epoch, last_loss, last_sari = load_model_checkpoint(model, optimizer)
        except:
            start_epoch = 0
            last_loss = float('inf')
            last_sari = 0

        # Training loop
        print("Starting training...")
        best_sari = last_sari
        patience_counter = 0

        for epoch in range(start_epoch, config.epochs):
            print(f'\nEpoch {epoch+1}/{config.epochs}')

            # Training
            train_loss, train_sari = train_epoch(train_data, valid_data, rl_agent, lex_sem_loss)

            # Validation
            val_loss, val_sari = evaluate_model(model, valid_data)

            # Update history
            history['train_loss'].append(train_loss)
            history['train_sari'].append(train_sari)
            history['val_loss'].append(val_loss)
            history['val_sari'].append(val_sari)

            print(f'Epoch {epoch+1}:')
            print(f'Train Loss: {train_loss:.4f}, Train SARI: {train_sari:.2f}')
            print(f'Val Loss: {val_loss:.4f}, Val SARI: {val_sari:.2f}')

            # Save checkpoint
            if (epoch + 1) % 5 == 0:
                save_model_checkpoint(model, epoch + 1, optimizer, val_loss, val_sari)

            # Save best model
            if val_sari > best_sari:
                best_sari = val_sari
                save_model_checkpoint(model, epoch + 1, optimizer, val_loss, val_sari, prefix='best')
                print(f"New best SARI: {best_sari:.2f} - Model saved!")
                patience_counter = 0
            else:
                patience_counter += 1

            # Early stopping
            if patience_counter >= config.patience:
                print("Early stopping triggered!")
                break

        # Final evaluation
        print("\nTraining completed! Evaluating on test set...")
        test_loss, test_sari = evaluate_model(model, test_data)
        print(f'Final Test Results:')
        print(f'Loss: {test_loss:.4f}')
        print(f'SARI: {test_sari:.2f}')

        # Plot training history
        plot_training_history(history)

        # Save final model
        model.save(os.path.join(config.model_dir, 'final_model'))
        print("Final model saved successfully!")

    except Exception as e:
        print(f"An error occurred during training: {str(e)}")
        raise











